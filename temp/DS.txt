DS

#3.practical of principle components analysis.

library(ggplot2)
data(iris)
X <- iris[, 1:4]
pca_result <- prcomp(X, center = TRUE, scale. = TRUE)
summary(pca_result)
biplot(pca_result, main = "PCA Biplot of Iris Dataset")
pca_data <- data.frame(pca_result$x, Species = iris $Species)
ggplot(pca_data, aes (x = PC1, y = PC2, color = Species)) +
  geom_point() +
  labs(title = "PCA of Iris Dataset", x = "Principal Component 1", y = "Principal
Component 2")
screeplot(pca_result, main = "Screen Plot",col="blue")


#4.practical of k means clustering.
# Load dataset
data("iris")

# Display column names
names(iris)

# Select all columns except 'Species'
new_data <- subset(iris, select = -Species)
new_data

# Apply k-means clustering with 3 clusters
cl <- kmeans(new_data, 3)
cl

# Copy new_data to another variable
data <- new_data

# Compute within-cluster sum of squares for different k values
wss <- sapply(1:15, function(k) {
  kmeans(data, k)$tot.withinss
})
wss

# Plot the Elbow Method graph
plot(1:15, wss,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of squares")

# Load required libraries
# install.packages("cluster")
library(cluster)

# install.packages("ggplot2")
library(ggplot2)

# Visualize clusters
clusplot(new_data, cl$cluster, color = TRUE, shade = TRUE, 
         labels = 2, lines = 0)

# Display cluster assignments
cl$cluster

# Display cluster centers
cl$centers


# 5. practical of hierarchical clustering.
library(ggplot2)
"agglomarative clustering "
clusters <- hclust(dist(iris[, 3:4]))
plot(clusters)
clusterCut <- cutree(clusters, 3)
table(clusterCut, iris$Species)
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) + 
  geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + 
  scale_color_manual(values = c('black', 'red', 'green'))

clusters <- hclust(dist(iris[, 3:4]), method = 'average')
clusterCut1 <- cutree(clusters, 3)
table(clusterCut1, iris$Species)

plot(clusters)
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) + 
  geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut1) + 
  scale_color_manual(values = c('black', 'red', 'green'))


#6. practical of time series forecasting
# Load the inbuilt dataset AirPassengers
data("AirPassengers")

# Check the class of the dataset (it belongs to time series format)
class(AirPassengers)

# Get the start time of the time series
start(AirPassengers)

# Get the end time of the time series
end(AirPassengers)

# Get the frequency of the time series (12 means it's a monthly time series)
frequency(AirPassengers)

# Summary statistics of the dataset
summary(AirPassengers)

# 1. Plot the Time Series Model
plot(AirPassengers)

# 2. Best Fit Line for Regression
abline(lm(AirPassengers ~ time(AirPassengers)))
# 3. Print Seasonal Cycle Across Years
cycle(AirPassengers)

# 4. Aggregate the Cycle and Display Trend Per Year
plot(aggregate(AirPassengers,FUN = mean))

# -----------------------------------
# 5. Generate a Box Plot for Seasonality Analysis
# -----------------------------------
boxplot(AirPassengers~cycle(AirPassengers))


# 7.practical to implement regression line
#Simple plot by considering in built data set mtcars
attach(mtcars)
plot(wt,mpg)
# plot is the function and wt and mpg are the attributes available 
# in mtcars dataset
abline(lm(mpg ~ wt))
# abline is used to draw line on plot and mpg (~) versus wt
title("Regression of MPG on Weight ")


# 8.practical of simple/multiple linear regression
#Consider some dataset
#The predictor vector (independent variable)
height <- c(151,174,138,186,128,136,179,163,152,131)
#The response vector (dependent variable)
weight <- c(63,81,56,91,47,57,76,72,62,48)
#apply lm function for linear regression
student <- lm(weight ~ height)
student
#Find the weight of a person with height 170
a <- data.frame(height = 170)
result <- predict(student,a)
print(result)
#To plot the data
plot(result,col="red",xlab="height",ylab="weight",pch=8)

#multiple linear regression
data(mtcars) #Load the built-in mtcars dataset
head(mtcars) #View the first few rows of the dataset
summary(mtcars) #Summary Statistics
model <- lm(mpg ~ wt + hp,data = mtcars) #Fit a multiple linear
# regression model
summary(model) # view the model summary
new_data <- data.frame(wt = c(3,2.5),hp= c(110,150))
predictions <- predict(model,new_data = new_data)
print(predictions)
plot(predictions,col="red",pch=8)


#9.practical to implement logistics regression
# Load the 'mtcars' dataset and display it for exploration
View(mtcars)
# The variable 'vs' in the 'mtcars' dataset indicates the engine type:
# vs = 0 (V-shaped engine), vs = 1 (Straight engine)

#===============####=================
# Building the first logistic regression model based on 'mpg'
model1 <- glm(formula = vs ~ mpg, data = mtcars, family = "binomial")  
summary(model1)

# Predicting the probability of a straight engine for a car with 20 mpg
predict(model1, data.frame(mpg = 20), type = "response")
# Predicting probabilities for a range of mpg values from 20 to 30
predict(model1, data.frame(mpg = c(20:30)), type = "response")

# Building the second logistic regression model based on 'hp'
model2 <- glm(formula = vs ~ hp, data = mtcars, family = "binomial")  
summary(model2)

# Predicting the probability of a straight engine for a car with 150 horsepower
predict(model2, data.frame(hp = 150), type = "response")
# Predicting probabilities for multiple horsepower values: 150, 100, 50
predict(model2, data.frame(hp = c(150, 100, 50)), type = "response")

# Building the third logistic regression model with two predictors: 'hp' and 'mpg'
model3 <- glm(formula = vs ~ hp + mpg, data = mtcars, family = "binomial") 
summary(model3)

#Display AIC of models
AIC(model1)
AIC(model2)
AIC(model3)


#10.practical to implement hypothesis testing.
# Defining sample vector
x <- rnorm(100)

# One Sample T-Test
t.test(x, mu = 5)

# 2 tow sample tt testing.
x <- rnorm(100)
y <- rnorm(100)

# Two Sample T-Test
t.test(x, y)

#3 directional hypothesis
x <- rnorm(100)

# Directional hypothesis testing
t.test(x, mu = 2, alternative = 'greater')

# 4 one sample μ test
x <- rnorm(100)

# One sample test
wilcox.test(x, exact = FALSE)

# 5 two sample μ test
x <- rnorm(100)
y <- rnorm(100)

# Two sample test 
wilcox.test(x, y)

# 6 corelation test
# Using mtcars dataset in R
cor.test(mtcars$mpg, mtcars$hp)


#11.practical of one way anova
#  One way anova
data1 <- read.csv(file.choose(),sep = ",",header = T)
names(data1)
summary(data1)
head(data1)
anv <- aov(formula = satindex~dept,data=data1)
summary(anv)


# 12.practical of two way anova
"two way anova"
data2<-read.csv(file.choose(),sep=",",header = T)
names(data2)
summary(data2)
anv1<-aov(formula = satindex~ dept+exp+dept*exp,data = data2)
summary(anv1)


# 13.practical to implement decision tree.
# Load necessary libraries
library(tree)
library(caret)
library(dplyr)

# Load the dataset
df <- read.csv("C:\\Users\\sejalpatil1728\\Downloads\\student_improved.csv")

# Convert categorical columns to factors
df$Gender <- as.factor(df$Gender)
df$ProgrammingLanguage <- as.factor(df$ProgrammingLanguage)
df$Grade <- as.factor(df$Grade)

# Split the data into training and testing sets (80% train, 20% test)
set.seed(42)  # For reproducibility
trainIndex <- createDataPartition(df$Grade, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData <- df[-trainIndex, ]

# Create the decision tree model using the tree package
model <- tree(Grade ~ Age + Gender + ProgrammingLanguage, data = trainData)

# Print the model summary
summary(model)

# Predict on the test data
predictions <- predict(model, testData, type = "class")

# Evaluate the model's accuracy
confusionMatrix(predictions, testData$Grade)

# Visualize the decision tree
plot(model)
text(model, pretty = 0)


# 14.practical to draw histogram , line plot
#Line Plot
#create some data in x
x <- c(1:5)
y <- x
#plotting symbol and color
par(pch = 22 ,col = "darkblue")
#all plots on one page
par(mfrow = c(2,4))
opts = c("p","l","o","b","c","s","S","h")
for(i in 1:length(opts)){
  heading = paste("type = ",opts[i])
  plot(x,y,type="n",main=heading)
  lines(x,y,type=opts[i])
}

# histogram
#Histogram in R using mtcars dataset
hist(mtcars$mpg)
#mtcars is the name of the dataset and mpg is attribute
#$ is used to mention that mpg is in mtcars,hist is the command
#for histogram no of bars in the histogram can be controlled
# and bars can be colored
hist(mtcars$mpg,breaks=20,col="green")


#15. 1 Scattered Diagram (ScatterPlot):
  
  #simple scatterplot from mtcars dataset
  attach(mtcars)
plot(wt,mpg,main="ScatterPlot Example",xlab="Car Weight",ylab="Miles Per Gallon",pch=19)

#Pie Chart:

# Pie Chart with percentages
slices <- c(20,20,30,30)
label <- c('INDIA','US','DUBAI','FRANCE')
pct <- round(slices/sum(slices)*100)
label <- paste(label,pct)
#add percents to labels
label <- paste(label,"%",sep="")
#ad % to labels
pie(slices,labels = label,col=rainbow(length(label)),main="Pie Chart of Countries")

#Box Plot:

#Box Plot
#Boxplot of MPG by car cylinders
boxplot(mpg ~ cyl,data = mtcars,main="Car Milage Data",
        xlab="Number of Cylinders",ylab="Miles per Gallon")